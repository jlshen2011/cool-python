{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters. Spark supports multiple widely used programming languages (Python, Java, Scala, and R), includes libraries for diverse tasks ranging from SQL to streaming and machine learning, and runs anywehere from a laoptop to a cluster of thousands of servers. \n",
    "\n",
    "### Spark Application\n",
    "Spark Applications consist of a driver process and a set of executor processes. \n",
    "\n",
    "\n",
    "### Driver\n",
    "The driver process runs your main() function, sits on a node in the cluster, and is responsible for maintaining infomration about the Spark Application, responding to a user's program or input; and anlyzing, distributing, and scheduling work across the executors. \n",
    "\n",
    "### Executor\n",
    "The executors are responsible for actually carrying out the work that the driver assigns them. Each executor is responsible for exeucting code assigned ot it by the driver, and reporting the state of the computation on that executor back to the driver node. \n",
    "\n",
    "### Cluster Manager\n",
    "The cluster manager controls physical machines and allocates resources to Spark Applications. This can be one of three core cluster managers: Spark's standalone cluster manager, YARN, or Mesos. \n",
    "\n",
    "### Partition\n",
    "A partition is a collection of rows that sits on one physical machine in your cluster.\n",
    "\n",
    "### Execution Modes\n",
    "* Cluster mode. In a cluster mode, a user submits a pre-complied JAR, Python or R script to a cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. The cluster manager is responsible for maintaining all Spark Application-related processes. \n",
    "* Client mode. Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses.\n",
    "* Local mode. It runs the entire Spark Application on a single machine. It achieves parallelism through threads on that single machine.\n",
    "\n",
    "### A Spark Job\n",
    "In general, there should be one Spark job for one action. Actions always return results. Each job breaks down into a series of stages, the number of which depends on how many shuffle operations need to take place.\n",
    "\n",
    "\n",
    "### Stages\n",
    "Stages in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines. In general, Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles. A shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key (which requires sending records with the same key to the same node). This type of repartitioning requires coordinating across executors to move data around. Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result.\n",
    "\n",
    "### Tasks\n",
    "Stages in Spark consist of tasks. Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor. If there is one big partition in our dataset, we will have one task. If there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel. A task is just a unit of computation applied to a unit of data (the partition).\n",
    "\n",
    "\n",
    "### Distribution of Executors, Cores and Memory for a Spark Application\n",
    "Theory:\n",
    "* Hadoop/Yarn/OS Deamons: When we run spark application using a cluster manager like Yarn, there’ll be several daemons that’ll run in the background. So, while specifying num-executors, we need to make sure that we leave aside enough cores (~1 core per node) for these daemons to run smoothly.\n",
    "* Yarn ApplicationMaster (AM): ApplicationMaster is responsible for negotiating resources from the ResourceManager and working with the NodeManagers to execute and monitor the containers and their resource consumption. If we are running spark on yarn, then we need to budget in the resources that AM would need (~1024MB and 1 Executor).\n",
    "* HDFS Throughput: HDFS client has trouble with tons of concurrent threads. It was observed that HDFS achieves full write throughput with ~5 tasks per executor. So it’s good to keep the number of cores per executor below that number. * Spark Yarn executor MemoryOverhead which is maxima of 384MB or 7% of the executor memory.\n",
    "\n",
    "Example: \n",
    "Consider a 10 node cluster with 16 cores and 64GB RAM per node.     \n",
    "    * Based on the recommendations mentioned above for good HDFS throughput, Let’s assign --executor-core = 5\n",
    "    * Leave 1 core per node for Hadoop/Yarn daemons => Num cores available per node = 16-1 = 15\n",
    "    * Total available of cores in cluster = 15 x 10 = 150\n",
    "    * Number of available executors = (total cores/num-cores-per-executor) = 150 / 5 = 30\n",
    "    * Leaving 1 executor for ApplicationManager => --num-executors = 29\n",
    "    * Number of executors per node = 30/10 = 3\n",
    "    * Memory per executor = 64GB/3 = 21GB\n",
    "    * Counting off heap overhead = 7% of 21GB = 3GB\n",
    "    * Actual --executor-memory = 21 - 3 = 18GB\n",
    "\n",
    "### Transformation\n",
    "A transformation instructs Spark how you would lije to modify a DataFrame to do what you want. There are two types of transformations: Narrow dependencies are those for which each input partition will contribute to only one output partition; a wide dependency style transformation will have input partitions contributing to many output partitions. With narrow transformations, Spark will automatically perform an operation called pipelining, meaning that if we specify multiple filters on DataFrames, they will all be performed in memory. For shuffles, Spark writes results to disk.\n",
    "\n",
    "### Lazy Evaluation\n",
    "Spark waits until the very last moment to execute the graph of computation instructions. This provides immense benefits because Spark can optimize the entire data flow from end to end. \n",
    "\n",
    "\n",
    "### Action\n",
    "An action instructs Spark to compute a result from a series of transformations. There are three kinds of actuibs:\n",
    "* Actions to view data in the console\n",
    "* Actions to collect data to native objects in teh respective language\n",
    "* Actions to write to output data \n",
    "\n",
    "### DataFrames and Datasets\n",
    "DataFrames and Datasets are distributed table-like collections with well-defined rows and columns. They represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output. When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. These represent plans of how to manipulate rows and columns to compute the user's desired result.\n",
    "\n",
    "The difference between DataFrames and Datasets lies in \"types\". Both have types though. For DataFrames, Spark maintains types completelt and only checks whether those types line up to those specified in the schema at runtime. Datasets, on the other hand, check whether types conform to the specificiation at complie time. Datasets are only available to JVM-based languages (Scala and Java). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Hello World\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"../data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, count(1)\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "    ORDER BY sum(count) DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
