{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Model for Movie Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model with a wide set of crossed-column (co-occurrence) features can memorize the feature interactions, while deep neural networks (DNN) can generalize the feature patterns through low-dimensional dense embeddings learned for the sparse features. [**Wide-and-deep**](https://arxiv.org/abs/1606.07792) learning jointly trains wide linear model and deep neural networks to combine the benefits of memorization and generalization for recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified based on https://github.com/microsoft/recommenders/blob/master/examples/00_quick_start/wide_deep_movielens.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_USER_COL = \"userID\"\n",
    "DEFAULT_ITEM_COL = \"itemID\"\n",
    "DEFAULT_RATING_COL = \"rating\"\n",
    "DEFAULT_LABEL_COL = \"label\"\n",
    "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
    "DEFAULT_PREDICTION_COL = \"prediction\"\n",
    "USER_COL = \"col_user\"\n",
    "ITEM_COL = \"col_item\"\n",
    "RATING_COL = \"col_rating\"\n",
    "PREDICT_COL = \"col_prediction\"\n",
    "DEFAULT_K = 10\n",
    "DEFAULT_THRESHOLD = 10\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('data/ml-1m/ratings.dat', sep=\"::\", header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.columns = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count    Dtype\n",
      "---  ------     --------------    -----\n",
      " 0   UserID     1000209 non-null  int64\n",
      " 1   MovieID    1000209 non-null  int64\n",
      " 2   Rating     1000209 non-null  int64\n",
      " 3   Timestamp  1000209 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 30.5 MB\n"
     ]
    }
   ],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('data/ml-1m/movies.dat', sep=\"::\", header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.columns = [\"MovieID\", \"Title\", \"Genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres: ['Action' 'Adventure' 'Animation' \"Children's\" 'Comedy' 'Crime'\n",
      " 'Documentary' 'Drama' 'Fantasy' 'Film-Noir' 'Horror' 'Musical' 'Mystery'\n",
      " 'Romance' 'Sci-Fi' 'Thriller' 'War' 'Western']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                              Genres  \n",
       "0  [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3  [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode 'genres' into int array (multi-hot representation) to use as item features\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "movies[\"Genres\"] = genres_encoder.fit_transform(movies[\"Genres\"].apply(lambda s: s.split(\"|\"))).tolist()\n",
    "print(\"Genres:\", genres_encoder.classes_)\n",
    "display(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(ratings[[\"UserID\", \"MovieID\", \"Rating\"]], movies[[\"MovieID\", \"Genres\"]], on=\"MovieID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating                                             Genres\n",
       "0       1     1193       5  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1       2     1193       5  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2      12     1193       4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3      15     1193       4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4      17     1193       5  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3706 items and 6040 users in the dataset\n"
     ]
    }
   ],
   "source": [
    "items = data.drop_duplicates(\"MovieID\")[[\"MovieID\", \"Genres\"]].reset_index(drop=True)\n",
    "item_feat_shape = len(items[\"Genres\"][0])\n",
    "users = data.drop_duplicates(\"UserID\")[[\"UserID\"]].reset_index(drop=True)\n",
    "print(\"Total {} items and {} users in the dataset\".format(len(items), len(users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and deep model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_columns(\n",
    "    users,\n",
    "    items,\n",
    "    user_col,\n",
    "    item_col,\n",
    "    item_feat_col=None,\n",
    "    crossed_feat_dim=1000,\n",
    "    user_dim=8,\n",
    "    item_dim=8,\n",
    "    item_feat_shape=None,\n",
    "    model_type=\"wide_deep\"\n",
    "):\n",
    "    \"\"\"Build wide and/or deep feature columns for TensorFlow high-level API Estimator.\n",
    "\n",
    "    Args:\n",
    "        users (iterable): Distinct user ids.\n",
    "        items (iterable): Distinct item ids.\n",
    "        user_col (str): User column name.\n",
    "        item_col (str): Item column name.\n",
    "        item_feat_col (str): Item feature column name for 'deep' or 'wide_deep' model.\n",
    "        crossed_feat_dim (int): Crossed feature dimension for 'wide' or 'wide_deep' model.\n",
    "        user_dim (int): User embedding dimension for 'deep' or 'wide_deep' model.\n",
    "        item_dim (int): Item embedding dimension for 'deep' or 'wide_deep' model.\n",
    "        item_feat_shape (int or an iterable of integers): Item feature array shape for 'deep' or 'wide_deep' model.\n",
    "        model_type (str): Model type, either\n",
    "            'wide' for a linear model,\n",
    "            'deep' for a deep neural networks, or\n",
    "            'wide_deep' for a combination of linear model and neural networks.\n",
    "\n",
    "    Returns:\n",
    "        list of tf.feature_column, list of tf.feature_column: Two lists. One with the wide feature columns and a second\n",
    "        with the deep feature columns. If only the wide model is selected, the deep column list is empty and viceversa. \n",
    "    \"\"\"\n",
    "    if model_type not in [\"wide\", \"deep\", \"wide_deep\"]:\n",
    "        raise ValueError(\"Model type should be either 'wide', 'deep', or 'wide_deep'\")\n",
    "\n",
    "    user_ids = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        user_col, users\n",
    "    )\n",
    "    item_ids = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        item_col, items\n",
    "    )\n",
    "\n",
    "    if model_type == \"wide\":\n",
    "        return _build_wide_columns(user_ids, item_ids, crossed_feat_dim), []\n",
    "    elif model_type == \"deep\":\n",
    "        return (\n",
    "            [],\n",
    "            _build_deep_columns(\n",
    "                user_ids, item_ids, user_dim, item_dim, item_feat_col, item_feat_shape\n",
    "            ),\n",
    "        )\n",
    "    elif model_type == \"wide_deep\":\n",
    "        return (\n",
    "            _build_wide_columns(user_ids, item_ids, crossed_feat_dim),\n",
    "            _build_deep_columns(\n",
    "                user_ids, item_ids, user_dim, item_dim, item_feat_col, item_feat_shape\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "def _build_wide_columns(user_ids, item_ids, hash_bucket_size=1000):\n",
    "    \"\"\"Build wide feature (crossed) columns. `user_ids` * `item_ids` are hashed into `hash_bucket_size`\n",
    "\n",
    "    Args:\n",
    "        user_ids (tf.feature_column.categorical_column_with_vocabulary_list): User ids.\n",
    "        item_ids (tf.feature_column.categorical_column_with_vocabulary_list): Item ids.\n",
    "        hash_bucket_size (int): Hash bucket size.\n",
    "\n",
    "    Returns:\n",
    "        list of tf.feature_column: Wide feature columns.\n",
    "    \"\"\"\n",
    "    # Including the original features in addition to the crossed one is recommended to address hash collision problem.\n",
    "    return [\n",
    "        user_ids,\n",
    "        item_ids,\n",
    "        tf.feature_column.crossed_column(\n",
    "            [user_ids, item_ids], hash_bucket_size=hash_bucket_size\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "def _build_deep_columns(\n",
    "    user_ids, item_ids, user_dim, item_dim, item_feat_col=None, item_feat_shape=1\n",
    "):\n",
    "    \"\"\"Build deep feature columns\n",
    "\n",
    "    Args:\n",
    "        user_ids (tf.feature_column.categorical_column_with_vocabulary_list): User ids.\n",
    "        item_ids (tf.feature_column.categorical_column_with_vocabulary_list): Item ids.\n",
    "        user_dim (int): User embedding dimension.\n",
    "        item_dim (int): Item embedding dimension.\n",
    "        item_feat_col (str): Item feature column name.\n",
    "        item_feat_shape (int or an iterable of integers): Item feature array shape.\n",
    "    \n",
    "    Returns:\n",
    "        list of tf.feature_column: Deep feature columns.\n",
    "    \"\"\"\n",
    "    deep_columns = [\n",
    "        # User embedding\n",
    "        tf.feature_column.embedding_column(\n",
    "            categorical_column=user_ids, dimension=user_dim, max_norm=user_dim ** 0.5\n",
    "        ),\n",
    "        # Item embedding\n",
    "        tf.feature_column.embedding_column(\n",
    "            categorical_column=item_ids, dimension=item_dim, max_norm=item_dim ** 0.5\n",
    "        ),\n",
    "    ]\n",
    "    # Item feature\n",
    "    if item_feat_col is not None:\n",
    "        deep_columns.append(\n",
    "            tf.feature_column.numeric_column(\n",
    "                item_feat_col, shape=item_feat_shape, dtype=tf.float32\n",
    "            )\n",
    "        )\n",
    "    return deep_columns\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    model_dir=\"model_checkpoints\",\n",
    "    wide_columns=(),\n",
    "    deep_columns=(),\n",
    "    linear_optimizer=\"Ftrl\",\n",
    "    dnn_optimizer=\"Adagrad\",\n",
    "    dnn_hidden_units=(128, 128),\n",
    "    dnn_dropout=0.0,\n",
    "    dnn_batch_norm=True,\n",
    "    log_every_n_iter=1000,\n",
    "    save_checkpoints_steps=10000,\n",
    "    seed=None,\n",
    "):\n",
    "    \"\"\"Build wide-deep model.\n",
    "    \n",
    "    To generate wide model, pass wide_columns only.\n",
    "    To generate deep model, pass deep_columns only.\n",
    "    To generate wide_deep model, pass both wide_columns and deep_columns.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): Model checkpoint directory.\n",
    "        wide_columns (list of tf.feature_column): Wide model feature columns.\n",
    "        deep_columns (list of tf.feature_column): Deep model feature columns.\n",
    "        linear_optimizer (str or tf.train.Optimizer): Wide model optimizer name or object.\n",
    "        dnn_optimizer (str or tf.train.Optimizer): Deep model optimizer name or object.\n",
    "        dnn_hidden_units (list of int): Deep model hidden units. E.g., [10, 10, 10] is three layers of 10 nodes each.\n",
    "        dnn_dropout (float): Deep model's dropout rate.\n",
    "        dnn_batch_norm (bool): Deep model's batch normalization flag.\n",
    "        log_every_n_iter (int): Log the training loss for every n steps.\n",
    "        save_checkpoints_steps (int): Model checkpoint frequency.\n",
    "        seed (int): Random seed.\n",
    "\n",
    "    Returns:\n",
    "        tf.estimator.Estimator: Model\n",
    "    \"\"\"\n",
    "    # TensorFlow training log frequency setup\n",
    "    config = tf.estimator.RunConfig(\n",
    "        tf_random_seed=seed,\n",
    "        log_step_count_steps=log_every_n_iter,\n",
    "        save_checkpoints_steps=save_checkpoints_steps,\n",
    "    )\n",
    "\n",
    "    if len(wide_columns) > 0 and len(deep_columns) == 0:\n",
    "        model = tf.estimator.LinearRegressor(\n",
    "            model_dir=model_dir,\n",
    "            config=config,\n",
    "            feature_columns=wide_columns,\n",
    "            optimizer=linear_optimizer,\n",
    "        )\n",
    "    elif len(wide_columns) == 0 and len(deep_columns) > 0:\n",
    "        model = tf.estimator.DNNRegressor(\n",
    "            model_dir=model_dir,\n",
    "            config=config,\n",
    "            feature_columns=deep_columns,\n",
    "            hidden_units=dnn_hidden_units,\n",
    "            optimizer=dnn_optimizer,\n",
    "            dropout=dnn_dropout,\n",
    "            batch_norm=dnn_batch_norm,\n",
    "        )\n",
    "    elif len(wide_columns) > 0 and len(deep_columns) > 0:\n",
    "        model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "            model_dir=model_dir,\n",
    "            config=config,\n",
    "            # wide settings\n",
    "            linear_feature_columns=wide_columns,\n",
    "            linear_optimizer=linear_optimizer,\n",
    "            # deep settings\n",
    "            dnn_feature_columns=deep_columns,\n",
    "            dnn_hidden_units=dnn_hidden_units,\n",
    "            dnn_optimizer=dnn_optimizer,\n",
    "            dnn_dropout=dnn_dropout,\n",
    "            batch_norm=dnn_batch_norm,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"To generate wide model, set wide_columns.\\n\"\n",
    "            \"To generate deep model, set deep_columns.\\n\"\n",
    "            \"To generate wide_deep model, set both wide_columns and deep_columns.\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS = dict(\n",
    "    adadelta=tf.keras.optimizers.Adadelta,\n",
    "    adagrad=tf.keras.optimizers.Adagrad,\n",
    "    adam=tf.keras.optimizers.Adam,\n",
    "    ftrl=tf.keras.optimizers.Ftrl,\n",
    "    rmsprop=tf.keras.optimizers.RMSprop,\n",
    "    sgd=tf.keras.optimizers.SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(name, lr=0.001, **kwargs):\n",
    "    \"\"\"Get an optimizer for TensorFlow high-level API Estimator.\n",
    "\n",
    "    Args:\n",
    "        name (str): Optimizer name. Note, to use 'Momentum', should specify\n",
    "        lr (float): Learning rate\n",
    "        kwargs: Optimizer arguments as key-value pairs\n",
    "\n",
    "    Returns:\n",
    "        tf.train.Optimizer\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "\n",
    "    try:\n",
    "        optimizer_class = OPTIMIZERS[name]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"Optimizer name should be one of: {}\".format(list(OPTIMIZERS)))\n",
    "\n",
    "    # Set parameters\n",
    "    params = {}\n",
    "    if name == \"ftrl\":\n",
    "        params[\"l1_regularization_strength\"] = kwargs.get(\n",
    "            \"l1_regularization_strength\", 0.0\n",
    "        )\n",
    "        params[\"l2_regularization_strength\"] = kwargs.get(\n",
    "            \"l2_regularization_strength\", 0.0\n",
    "        )\n",
    "    elif name == \"rmsprop\":\n",
    "        params[\"momentum\"] = kwargs.get(\"momentum\", 0.0)\n",
    "\n",
    "    return optimizer_class(learning_rate=lr, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wide (linear) and deep (dnn) features\n",
    "wide_columns, deep_columns = build_feature_columns(\n",
    "    users=users[\"UserID\"].values,\n",
    "    items=items[\"MovieID\"].values,\n",
    "    user_col=\"UserID\",\n",
    "    item_col=\"MovieID\",\n",
    "    item_feat_col=\"Genres\",\n",
    "    crossed_feat_dim=1000,\n",
    "    user_dim=32,\n",
    "    item_dim=16,\n",
    "    item_feat_shape=item_feat_shape,\n",
    "    model_type=\"wide_deep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide feature specs:\n",
      "\t VocabularyListCategoricalColumn(key='UserID', vocabulary_list=(1, 2, 12, 15, 17, 18, 19, 24, 28, 33, ...\n",
      "\t VocabularyListCategoricalColumn(key='MovieID', vocabulary_list=(1193, 661, 914, 3408, 2355, 1197, 12 ...\n",
      "\t CrossedColumn(keys=(VocabularyListCategoricalColumn(key='UserID', vocabulary_list=(1, 2, 12, 15, 17, ...\n",
      "Deep feature specs:\n",
      "\t EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='UserID', vocabulary_list=(1, ...\n",
      "\t EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='MovieID', vocabulary_list=(1 ...\n",
      "\t NumericColumn(key='Genres', shape=(18,), default_value=None, dtype=tf.float32, normalizer_fn=None) ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Wide feature specs:\")\n",
    "for c in wide_columns:\n",
    "    print(\"\\t\", str(c)[:100], \"...\")\n",
    "print(\"Deep feature specs:\")\n",
    "for c in deep_columns:\n",
    "    print(\"\\t\", str(c)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TMP_DIR = TemporaryDirectory()\n",
    "model_dir = TMP_DIR.name\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    model_dir=model_dir,\n",
    "    wide_columns=wide_columns,\n",
    "    deep_columns=deep_columns,\n",
    "    linear_optimizer=build_optimizer('adagrad', 0.0621, **{\n",
    "        'l1_regularization_strength': 0,\n",
    "        'l2_regularization_strength': 0,\n",
    "        'momentum': 0,\n",
    "    }),\n",
    "    dnn_optimizer=build_optimizer('adadelta', 0.1, **{\n",
    "        'l1_regularization_strength': 0,\n",
    "        'l2_regularization_strength': 0,\n",
    "        'momentum': 0,  \n",
    "    }),\n",
    "    dnn_hidden_units=[h for h in [0, 64, 128, 512] if h > 0],\n",
    "    dnn_dropout=0.8,\n",
    "    dnn_batch_norm=True,\n",
    "    log_every_n_iter=max(1, 50000//10),  # log 10 times\n",
    "    save_checkpoints_steps=max(1, 50000 // 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by(df, filter_by_df, filter_by_cols):\n",
    "    \"\"\"From the input DataFrame (df), remove the records whose target column (filter_by_cols) values are\n",
    "    exist in the filter-by DataFrame (filter_by_df)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Source dataframe.\n",
    "        filter_by_df (pd.DataFrame): Filter dataframe.\n",
    "        filter_by_cols (iterable of str): Filter columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe filtered by filter_by_df on filter_by_cols\n",
    "    \"\"\"\n",
    "\n",
    "    return df.loc[\n",
    "        ~df.set_index(filter_by_cols).index.isin(\n",
    "            filter_by_df.set_index(filter_by_cols).index\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def user_item_pairs(\n",
    "    user_df,\n",
    "    item_df,\n",
    "    user_col,\n",
    "    item_col,\n",
    "    user_item_filter_df=None,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "):\n",
    "    \"\"\"Get all pairs of users and items data.\n",
    "\n",
    "    Args:\n",
    "        user_df (pd.DataFrame): User data containing unique user ids and maybe their features.\n",
    "        item_df (pd.DataFrame): Item data containing unique item ids and maybe their features.\n",
    "        user_col (str): User id column name.\n",
    "        item_col (str): Item id column name.\n",
    "        user_item_filter_df (pd.DataFrame): User-item pairs to be used as a filter.\n",
    "        shuffle (bool): If True, shuffles the result.\n",
    "        seed (int): Random seed for shuffle\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: All pairs of user-item from user_df and item_df, excepting the pairs in user_item_filter_df\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all user-item pairs\n",
    "    user_df[\"key\"] = 1\n",
    "    item_df[\"key\"] = 1\n",
    "    users_items = user_df.merge(item_df, on=\"key\")\n",
    "\n",
    "    user_df.drop(\"key\", axis=1, inplace=True)\n",
    "    item_df.drop(\"key\", axis=1, inplace=True)\n",
    "    users_items.drop(\"key\", axis=1, inplace=True)\n",
    "\n",
    "    # Filter\n",
    "    if user_item_filter_df is not None:\n",
    "        users_items = filter_by(users_items, user_item_filter_df, [user_col, item_col])\n",
    "\n",
    "    if shuffle:\n",
    "        users_items = users_items.sample(frac=1, random_state=seed).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "    return users_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_pool = user_item_pairs(\n",
    "    user_df=users,\n",
    "    item_df=items,\n",
    "    user_col=\"UserID\",\n",
    "    item_col=\"MovieID\",\n",
    "    user_item_filter_df=train,  # Remove seen items\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21634084, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dataset(x, y=None, batch_size=128, num_epochs=1, shuffle=False, seed=None):\n",
    "    if y is None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(\n",
    "            1000, seed=seed, reshuffle_each_iteration=True  # buffer size = 1000\n",
    "        )\n",
    "    elif seed is not None:\n",
    "        import warnings\n",
    "\n",
    "        warnings.warn(\"Seed was set but `shuffle=False`. Seed will be ignored.\")\n",
    "\n",
    "    return dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "\n",
    "def pandas_input_fn(\n",
    "    df, y_col=None, batch_size=128, num_epochs=1, shuffle=False, seed=None\n",
    "):\n",
    "    \"\"\"Pandas input function for TensorFlow high-level API Estimator.\n",
    "    This function returns a `tf.data.Dataset` function.\n",
    "\n",
    "    .. note::\n",
    "    \n",
    "        `tf.estimator.inputs.pandas_input_fn` cannot handle array/list column properly.\n",
    "        For more information, see https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data containing features.\n",
    "        y_col (str): Label column name if df has it.\n",
    "        batch_size (int): Batch size for the input function.\n",
    "        num_epochs (int): Number of epochs to iterate over data. If None will run forever.\n",
    "        shuffle (bool): If True, shuffles the data queue.\n",
    "        seed (int): Random seed for shuffle.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset function\n",
    "    \"\"\"\n",
    "\n",
    "    X_df = df.copy()\n",
    "    if y_col is not None:\n",
    "        y = X_df.pop(y_col).values\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    X = {}\n",
    "    for col in X_df.columns:\n",
    "        values = X_df[col].values\n",
    "        if isinstance(values[0], (list, np.ndarray)):\n",
    "            values = np.array([l for l in values], dtype=np.float32)\n",
    "        X[col] = values\n",
    "\n",
    "    return lambda: _dataset(\n",
    "        x=X,\n",
    "        y=y,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = pandas_input_fn(\n",
    "    df=train,\n",
    "    y_col=\"Rating\",\n",
    "    batch_size=32,\n",
    "    num_epochs=None,  # We use steps=TRAIN_STEPS instead.\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps = 50000, Batch size = 32 (num epochs = 0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /Users/jielishen/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1481: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/jielishen/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:83: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 15.009094, step = 0\n",
      "INFO:tensorflow:global_step/sec: 517.555\n",
      "INFO:tensorflow:loss = 0.85233754, step = 5000 (9.661 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10000...\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10000...\n",
      "INFO:tensorflow:global_step/sec: 520.732\n",
      "INFO:tensorflow:loss = 0.8221304, step = 10000 (9.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.098\n",
      "INFO:tensorflow:loss = 0.660743, step = 15000 (9.040 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 20000...\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into /var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 20000...\n",
      "INFO:tensorflow:global_step/sec: 519.467\n",
      "INFO:tensorflow:loss = 0.8648975, step = 20000 (9.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 555.127\n",
      "INFO:tensorflow:loss = 0.73422706, step = 25000 (9.007 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 30000...\n",
      "INFO:tensorflow:Saving checkpoints for 30000 into /var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 30000...\n",
      "INFO:tensorflow:global_step/sec: 519.346\n",
      "INFO:tensorflow:loss = 0.84727657, step = 30000 (9.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.744\n",
      "INFO:tensorflow:loss = 0.65210533, step = 35000 (9.013 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 40000...\n",
      "INFO:tensorflow:Saving checkpoints for 40000 into /var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 40000...\n",
      "INFO:tensorflow:global_step/sec: 522.038\n",
      "INFO:tensorflow:loss = 0.6470281, step = 40000 (9.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 542.208\n",
      "INFO:tensorflow:loss = 0.7172744, step = 45000 (9.222 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 50000...\n",
      "INFO:tensorflow:Saving checkpoints for 50000 into /var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm/model.ckpt.\n",
      "WARNING:tensorflow:From /Users/jielishen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 50000...\n",
      "INFO:tensorflow:Loss for final step: 0.57369435.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training steps = {}, Batch size = {} (num epochs = {})\".format(50000, 32, 5000 * 32 // len(train)))\n",
    "\n",
    "try:\n",
    "    model.train(input_fn=train_fn, steps=50000)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(model.predict(input_fn=pandas_input_fn(df=test)))\n",
    "prediction_df = test.drop(\"Rating\", axis=1)\n",
    "prediction_df[\"Rating\"] = [p['predictions'][0] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8520599047418799"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(test[\"Rating\"], prediction_df[\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7395462591053145"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(test[\"Rating\"], prediction_df[\"Rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend k items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/45/q__m21f163d57l7zhtj1yvvc0000gn/T/tmpkk9yzulm/model.ckpt-50000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = list(model.predict(input_fn=pandas_input_fn(df=ranking_pool)))\n",
    "prediction_df = ranking_pool.copy()\n",
    "prediction_df[\"Rating\"] = [p['predictions'][0] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           3.125630\n",
       "1           3.579156\n",
       "2           3.032273\n",
       "3           2.844368\n",
       "4           3.249866\n",
       "              ...   \n",
       "21634079    3.434063\n",
       "21634080    3.047624\n",
       "21634081    3.225728\n",
       "21634082    2.947160\n",
       "21634083    3.550170\n",
       "Name: Rating, Length: 21634084, dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df[\"Rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_items(\n",
    "    dataframe, col_user, col_rating, k\n",
    "):\n",
    "    \"\"\"Get the input customer-item-rating tuple in the format of Pandas\n",
    "    DataFrame, output a Pandas DataFrame in the dense format of top k items\n",
    "    for each user.\n",
    "    \n",
    "    Note:\n",
    "        If it is implicit rating, just append a column of constants to be\n",
    "        ratings.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n",
    "        customerID-itemID-rating)\n",
    "        col_user (str): column name for user\n",
    "        col_rating (str): column name for rating\n",
    "        k (int or None): number of items for each user; None means that the input has already been\n",
    "        filtered out top k items and sorted by ratings and there is no need to do that again.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of top k items for each user, sorted by `col_user` and `rank`\n",
    "    \"\"\"\n",
    "    # Sort dataframe by col_user and (top k) col_rating\n",
    "    if k is None:\n",
    "        top_k_items = dataframe\n",
    "    else:\n",
    "        top_k_items = (\n",
    "            dataframe.groupby(col_user, as_index=False)\n",
    "            .apply(lambda x: x.nlargest(k, col_rating))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    # Add ranks\n",
    "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
    "    return top_k_items\n",
    "\n",
    "\n",
    "def merge_ranking_true_pred(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user,\n",
    "    col_item,\n",
    "    col_rating,\n",
    "    col_prediction,\n",
    "    relevancy_method,\n",
    "    k,\n",
    "    threshold,\n",
    "):\n",
    "    \"\"\"Filter truth and prediction data frames on common users\n",
    "\n",
    "    Args:\n",
    "        rating_true (pd.DataFrame): True DataFrame\n",
    "        rating_pred (pd.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the \n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user (optional)\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame, int: DataFrame of recommendation hits, sorted by `col_user` and `rank`\n",
    "        DataFrmae of hit counts vs actual relevant items per user number of unique user ids\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the prediction and true data frames have the same set of users\n",
    "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
    "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
    "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
    "    n_users = len(common_users)\n",
    "\n",
    "    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n",
    "    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n",
    "    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n",
    "    # to calculate penalized precision of the ordered items.\n",
    "    if relevancy_method == \"top_k\":\n",
    "        top_k = k\n",
    "    elif relevancy_method == \"by_threshold\":\n",
    "        top_k = threshold\n",
    "    elif relevancy_method is None:\n",
    "        top_k = None\n",
    "    else:\n",
    "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
    "    df_hit = get_top_k_items(\n",
    "        dataframe=rating_pred_common,\n",
    "        col_user=col_user,\n",
    "        col_rating=col_prediction,\n",
    "        k=top_k,\n",
    "    )\n",
    "    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[\n",
    "        [col_user, col_item, \"rank\"]\n",
    "    ]\n",
    "\n",
    "    # count the number of hits vs actual relevant items per user\n",
    "    df_hit_count = pd.merge(\n",
    "        df_hit.groupby(col_user, as_index=False)[col_user].agg({\"hit\": \"count\"}),\n",
    "        rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n",
    "            {\"actual\": \"count\"}\n",
    "        ),\n",
    "        on=col_user,\n",
    "    )\n",
    "\n",
    "    return df_hit, df_hit_count, n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "    rating_true=test,\n",
    "    rating_pred=prediction_df,\n",
    "    col_user=\"UserID\",\n",
    "    col_item=\"MovieID\",\n",
    "    col_rating=\"Rating\",\n",
    "    col_prediction=\"Rating\",\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=10,\n",
    "    threshold=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10541205689683304"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalized Discounted Cumulative Gain (nDCG).\n",
    "\n",
    "# calculate discounted gain for hit items\n",
    "df_dcg = df_hit.copy()\n",
    "# relevance in this case is always 1\n",
    "df_dcg[\"dcg\"] = 1 / np.log1p(df_dcg[\"rank\"])\n",
    "# sum up discount gained to get discount cumulative gain\n",
    "df_dcg = df_dcg.groupby(\"UserID\", as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
    "# calculate ideal discounted cumulative gain\n",
    "df_ndcg = pd.merge(df_dcg, df_hit_count, on=[\"UserID\"])\n",
    "df_ndcg[\"idcg\"] = df_ndcg[\"actual\"].apply(\n",
    "    lambda x: sum(1 / np.log1p(range(1, min(x, 10) + 1)))\n",
    ")\n",
    "\n",
    "# DCG over IDCG is the normalized DCG\n",
    "(df_ndcg[\"dcg\"] / df_ndcg[\"idcg\"]).sum() / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10006622516556293"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision at k\n",
    "(df_hit_count[\"hit\"] / 10).sum() / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
